---
title: "Workflow through Model Fitting"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir="/home/claire/Git/PhenoRepo/")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

### Packages
```{r packages, message=FALSE}
rm(list=ls())

library(here)
library(lubridate)
library(tidyverse)
library(neon4cast)
library(neonUtilities)
library(zoo)
library(imputeTS)
library(forecast)

source("R/calcGDDfun.R")
source("R/WarmModel.R")
source("R/WarmModelForecast.R")
source("R/gridsearch_Casey.R")
source("R/ssq_phenomod.R")
source("R/nll_phenomod.R")
```

## Update and read-in target data, then remove leading NAs and fill in others
```{r targetDate}
# Update target data if not yet updated today 
target_fp <- "data/pheno/phenology-targets.csv.gz"

if(as.Date(file.info(target_fp)$ctime)!=Sys.Date()) {
  print("Downloading updated target data")
    download.file("https://data.ecoforecast.org/targets/phenology/phenology-targets.csv.gz",
              target_fp)
    }

targets_raw <- read.csv(target_fp,header=TRUE)
targets_raw$siteID <- as.factor(targets_raw$siteID)
targets_raw$time <- as.Date(targets_raw$time)

sites <- unique(targets_raw$siteID)
targets <- NULL

# Remove rows with leading NAs and fill in other NAs in gcc_90
for(site in sites){
  site_targs <- filter(targets_raw,siteID==site)
  # Remove rows with leading NAs
  while(is.na(site_targs$gcc_90[1])){
    site_targs <- site_targs[2:nrow(site_targs),]
  }

  if(site=='UKFS') {site_targs$gcc_90 <- tsclean(site_targs$gcc_90,replace.missing = F,lambda = 'auto') %>% as.numeric()}
 
  # Fill in other NAs
  x <- zoo(site_targs$gcc_90,site_targs$time)
  x <- na.interp(site_targs$gcc_90) %>% as.numeric()
  site_targs$gcc_90 <- x

  targets <- rbind(targets,site_targs)
  rm(site_targs,x)
}

targets$time <- as.Date(targets$time)
targets$day <- yday(targets$time)
targets$year <- substr(targets$time,1,4)

# Remove extra vars from Global Env
rm(target_fp,site,targets_raw)
```


## Update or read in temperature data
```{r NEONtempdata}

## Update NEON temperature data and read in. This will take A WHILE (2 hrs?), so only run this line if you really want to update weather data, otherwise keep it commented out and run the line the reads in the most recent temperature data

# source("R/GetNEONwxdata.R")
neon_wx <- read_csv("data/drivers/neon/temps_allsites.csv") %>% 
  select(siteID,date,daily_mean,daily_min,daily_max) %>% 
  mutate(source='neon') %>% 
  filter(date >= min(targets$time))

## Update NOAA weather data. This will take a minute or two, but shouldn't take longer
## When it finishes, should be a dataframe called noaa_wx in the global env
# source("R/GetNOAAForecastData.R")

## Or read in most recent pull of data
noaa_wx <- read_csv("./data/drivers/noaa/noaa_temp_4cast_2021-05-05.csv")
noaa_wx <- noaa_wx %>% 
  select(siteID,date,daily_mean,daily_min,daily_max) %>% 
  mutate(source='noaa')

all_wx <- rbind(neon_wx,noaa_wx)
rm(neon_wx,noaa_wx)

# Apply GDD function which will fill in missing temp data and calculate GDD values
GDD <- calcGDDfun(temp_df=all_wx,targets_df = targets,int_method = "spline")

# Add in day of year
GDD$day <- yday(GDD$date)

# For each site, check that target dates have matching weather dates
for(site in sites){
  
  targs <- filter(targets,siteID==site & time <= Sys.Date())
  wx <- filter(GDD,siteID==site & date <= Sys.Date())
  x = as.Date(setdiff(as.Date(targs$time),as.Date(wx$date)))
  
  if(length(x)>0) print(paste0("Targets for ",site," missing wx for",x))
  
}
rm(all_wx,wx,x,site)
```


## I. Model fitting   
### a. Parameter fitting
```{r gridsearch}

sites <- unique(targets$siteID) %>% as.character()
#sites = "GRSM"
site = sites[6]
# Start and end dates of warm period
spring_date <- "-02-24"
fall_date <- "-09-01"
# Dataframe to store fitted parameters for each site
params_df <- data.frame(site = character(),
                        G_init = double(),
                        a = double(),
                        b = double(),
                        t1 = double(),
                        t2 =  double())

# Start parameter fitting loop
for(site in sites){

  # Filter target data for site and warm period
  site_targs <- targets %>% 
    filter(siteID==site) %>% # Get site data
    arrange(time) %>%  # make sure arranged by time
    filter(time>=as.Date(paste0(year(time),spring_date)),
           time<=as.Date(paste0(year(time),fall_date)))
  
  # Filter weather data for site and warm period
  site_GDD <- GDD %>% 
    filter(siteID==site) %>% # Get site data
    arrange(date) %>%  # make sure arranged by time
    filter(date>=as.Date(paste0(year(date),spring_date)),
           date<=as.Date(paste0(year(date),fall_date)),
           date<=max(site_targs$time))

  # Remove 2017 for GRSM 
  if (site == "GRSM"){
    site_targs <- filter(site_targs, time >= as.Date("01-01-18","%m-%d-%y"))
    site_GDD <- filter(site_GDD, date >= as.Date("01-01-18","%m-%d-%y"))
  }

  # Find average GCC value in first seven days
  avg_G_init = mean(site_targs$gcc_90[1:7])
  
  # Find day of year (across years) when highest GCC occurs 
  avg_t2_day = site_targs %>% 
    group_by(year) %>% 
    filter(year != 2021) %>% # Maybe don't filter this for all sites? 
    summarise(maxGCC=max(gcc_90)) %>% 
    summarise(avg_maxGCC=mean(maxGCC)) %>% 
    as.numeric()
  
  # Find the average number of GDD days that has occurred up to that point
  #avg_t2 <- filter(site_GDD,day==avg_t2_day) %>% 
  #  summarise(mean(GDDdays)) %>% as.numeric()
  
  # list of parameter ranges                             
  pvecs <- list(G_init = avg_G_init,  # GCC guess = avg first 7 days of targets
              a=seq(0,0.001,length.out=10),   # green-up: fast growth
              b=seq(0,0.0001,length.out=10),  # maturation
              t1=seq(30,70,length.out=10),   # Spring transition
              t2 = avg_t2_day)  # Summer transition = avg date of peak GCC
  
  # First pass at parameter fitting using gridsearch and SSQ
  gridsearch_fit <- gridsearch(pvecs, ssq_phenmod, y=site_targs$gcc_90,GDD=site_GDD,spring_date=spring_date)
  
  # SSQ: Set of starting parameters for optim to use
  # SSQ OPTIM FIT
  optim_starts <- c(gridsearch_fit$par["G_init"],
                    gridsearch_fit$par["a"],
                    gridsearch_fit$par["b"],
                    gridsearch_fit$par["t1"],
                    gridsearch_fit$par["t2"])
  optim_fit <- optim(optim_starts,ssq_phenmod,y=site_targs$gcc_90, GDD = site_GDD,spring_date=spring_date)
  
  # Store results in dataframe
  params_df$site <- site
  params_df$G_init = optim_fit$par["G_init"]
  params_df$a = optim_fit$par["a"]
  params_df$b = optim_fit$par["b"]
  params_df$t1 = optim_fit$par["t1"]
  params_df$t2 = optim_fit$par["t2"]
  
  # Plot results
  # GDD Warm Model Results
  G_init = optim_fit$par["G_init"]
  a = optim_fit$par["a"]
  b = optim_fit$par["b"]
  t1 = optim_fit$par["t1"]
  t2 = optim_fit$par["t2"]
  
  model_results <-  WarmModel(site_GDD,G_init,a,b,t1,t2,spring_date = spring_date)
  colnames(model_results)[2] <- "gcc_90"
  model_results$day <- yday(model_results$date)
  ggplot() +
    geom_point(data = site_targs, aes(x = as.Date(time), y = gcc_90), color = "springgreen4") +
    geom_point(data=model_results, aes(x = date, y = gcc_90), color = "red") +
    labs(x="Day of year",y="GCC 90",title="1/days") +
    theme_classic(base_size = 15)
}


```

### B. Uncertainty estimation with cross validation
```{r loocv}

# sites <- unique(targets$siteID) %>% as.character()
sites = "GRSM"
# site = sites[6]
# Start and end dates of warm period
spring_date <- "-02-24"
fall_date <- "-09-01"
# Dataframe to store fitted parameters for each site
cv_params_df <- NULL
error_df <- NULL

# Start parameter fitting loop
for(site in sites){
  
  # Filter target data for site and warm period
  site_targs <- targets %>% 
    filter(siteID==site) %>% # Get site data
    arrange(time) %>%  # make sure arranged by time
    filter(time>=as.Date(paste0(year(time),spring_date)),
           time<=as.Date(paste0(year(time),fall_date)))
  
  # Filter weather data for site and warm period
  site_GDD <- GDD %>% 
    filter(siteID==site) %>% # Get site data
    arrange(date) %>%  # make sure arranged by time
    filter(date>=as.Date(paste0(year(date),spring_date)),
           date<=as.Date(paste0(year(date),fall_date)),
           date<=max(site_targs$time))

  # Remove 2017 for GRSM 
  if (site == "GRSM"){
    site_targs <- filter(site_targs, time >= as.Date("01-01-18","%m-%d-%y"))
    site_GDD <- filter(site_GDD, date >= as.Date("01-01-18","%m-%d-%y"))
  }

  years <- unique(site_targs$year)
  for(yeari in years){
  
    training_targs <- filter(site_targs,year!=yeari)
    test_targs <- filter(site_targs,year==yeari)
    
    training_GDD <- filter(site_GDD,year!=yeari)
    test_GDD <- filter(site_GDD,year==yeari)
    
    # Find average GCC value in first seven days
    avg_G_init = mean(training_targs$gcc_90[1:7])
    
    # Find day of year (across years) when highest GCC occurs 
    avg_t2_day = site_targs %>% 
      group_by(year) %>% 
      filter(year != 2021) %>% # Maybe don't filter this for all sites? 
      summarise(maxGCCday=day[gcc_90==max(gcc_90)]) %>% 
      summarise(avg_maxGCCday=floor(mean(maxGCCday))) %>% 
      as.numeric()
      
    # Find the average number of GDD days that has occurred up to that point
    avg_t2 <- filter(site_GDD,day==avg_t2_day) %>%
     summarise(mean(GDDdays)) %>% as.numeric()
    
    # list of parameter ranges                             
    pvecs <- list(G_init = avg_G_init,  # GCC guess = avg first 7 days of targets
                a=seq(0,0.001,length.out=10),   # green-up: fast growth
                b=seq(0,0.0001,length.out=10),  # maturation
                t1=seq(10,70,length.out=10),   # Spring transition
                t2 = avg_t2)  # Summer transition = avg date of peak GCC
    
    # First pass at parameter fitting using gridsearch and SSQ
    gridsearch_fit <- gridsearch(pvecs, ssq_phenmod,y=training_targs$gcc_90,GDD=training_GDD,spring_date=spring_date)
    
    # SSQ: Set of initial parameters for optim to use
    optim_starts <- c(gridsearch_fit$par["G_init"],
                      gridsearch_fit$par["a"],
                      gridsearch_fit$par["b"],
                      gridsearch_fit$par["t1"],
                      gridsearch_fit$par["t2"])
    optim_fit <- optim(optim_starts,ssq_phenmod,y=training_targs$gcc_90, GDD = training_GDD,spring_date=spring_date)
    
    # Store parameters in data frame
    tmp_params <- data.frame(siteID=site,
                             test_year = yeari,
                             G_init = optim_fit$par["G_init"],
                             a = optim_fit$par["a"],
                             b = optim_fit$par["b"],
                             t1 = optim_fit$par["t1"],
                             t2 = optim_fit$par["t2"])
    cv_params_df <- rbind(cv_params_df,tmp_params)
    
    # Make list of parameters to feed into WarmModelForecast function
    params <- list(G_init = optim_fit$par["G_init"],
                   a = optim_fit$par["a"],
                   b = optim_fit$par["b"],
                   t1 = optim_fit$par["t1"],
                   t2 = optim_fit$par["t2"])
    
    yeari_forecast <- WarmModelForecast(params=params,
                                        test_GDD = test_GDD,
                                        test_targs = test_targs,
                                        spring_date = spring_date)
    
    error_vec <- if(is.null(nrow(error_df))){
      yeari_forecast$error
    }else if(nrow(yeari_forecast)<nrow(error_df)) {
      c(yeari_forecast$error,rep(NA,(nrow(error_df)-nrow(yeari_forecast))))
    } else {
        yeari_forecast$error
      }
                    
    error_df <- cbind(error_df,yeari_forecast$error)
    colnames(error_df)[ncol(error_df)] <- c(paste0(site,as.character(yeari)))
    
    }

  }

```


```{r}
ggplot(yeari_forecast,aes(x=time))+
  geom_point(aes(y=obs_gcc_90))+
  geom_point(aes(y=pred_gcc_90))+
  # geom_ribbon((aes(ymax=pred_gcc_90+abs(deviation),ymin=pred_gcc_90-abs(deviation))),alpha=0.5)+
  theme_classic()
```


# Uncertainty calculated for all prior years and tested on 2020
model_results$day <- yday(model_results$date)
model_results$error <- NA

# Calculate daily StDev from observed data and model prediction
for (d in 1:244){
  target_unc <- site_targs %>% filter(day == d & time < as.Date("01-01-20","%m-%d-%y") )
  model_unc <- model_results %>% filter(day == d & date < as.Date("01-01-20","%m-%d-%y") )
  ssq = 0
  for (i in 1:nrow(target_unc)){
    ssq = ssq + (target_unc$gcc_90[i] - model_unc$gcc_90[i])^2
  }
  error = sqrt(ssq/nrow(target_unc))
  model_results$error[model_results$day==d]=error  
}
# Calculate 7 day rolling average of error
model_results <- model_results %>%
  arrange(date) %>%
  mutate(error = rollmean(error, k=3, fill = NA))


ggplot() +
    #geom_ribbon(data = model_results, aes(x = date, ymin = gcc_90 - error, ymax = gcc_90 + error), color = "gray") +
    geom_point(data = site_targs, aes(x = as.Date(time), y = gcc_90), color = "springgreen4") +
    geom_point(data=model_results, aes(x = date, y = gcc_90), color = "red") +
    labs(x="Day of year",y="GCC 90",title="1/days") +
    theme_classic(base_size = 15)

